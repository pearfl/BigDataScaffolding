# === 组件声明 ===
client.sources = Taildir_Source
client.channels = File_Channel
client.sinks = Kafka_Sink

# === Source: Taildir_Source (TAILDIR) ===
# 源类型：实时监控新增日志
client.sources.Taildir_Source.type = TAILDIR
client.sources.Taildir_Source.filegroups = qxbOpenAPI
# 监控的日志文件路径（正则表达式匹配）
client.sources.Taildir_Source.filegroups.qxbOpenAPI = /var/log/openServerLogs/qxbOpenAPI/.*\.log
# 优化建议：设为true可在Kafka识别来源
client.sources.Taildir_Source.fileHeader = true
client.sources.Taildir_Source.monitoringInterval = 5000
# 存储文件读取位置信息（建议目录预先创建）
client.sources.Taildir_Source.positionFile = /flumedata/mrs/flume/qxbOpenAPI/taildir/taildir_position.json
# 警告：未设置目录扫描间隔（默认60s，建议添加：monitoringInterval=5000）
client.sources.Taildir_Source.montime =
# 不在事件头中存储字节偏移量
client.sources.Taildir_Source.byteOffsetHeader = false
# 新文件不从末尾读取（从位置记录点开始）
client.sources.Taildir_Source.skipToEnd = false
# 12秒无新数据触发超时（单位毫秒）
client.sources.Taildir_Source.idleTimeout = 12000
# 3秒写入一次位置信息
client.sources.Taildir_Source.writePosInterval = 3000
# 每次最多读取1000个事件
client.sources.Taildir_Source.batchSize = 1000
# 启用fileHeader时的key名称
client.sources.Taildir_Source.fileHeaderKey = file
# 绑定输出通道
client.sources.Taildir_Source.channels = File_Channel

# === Channel: File_Channel (原始图片此处有严重前缀错误) ===
# 通道类型：文件通道（高可靠性）
client.channels.File_Channel.type = file
# 数据存储目录（需确保磁盘空间充足）
client.channels.File_Channel.dataDirs = /flumedata/mrs/flume/qxbOpenAPI/data
# 检查点存储目录
client.channels.File_Channel.checkpointDir = /flumedata/mrs/flume/qxbOpenAPI/checkpoint
# 通道最大容量100万个事件
client.channels.File_Channel.capacity = 1000000
# 错误参数：应为keep-alive（此处参数无效）
client.channels.File_Channel.channelfullcount = 10
# 禁用双检查点机制（有数据丢失风险）
client.channels.File_Channel.useDualCheckpoints = false
# 警告：未设置备份检查点目录（建议启用双检查点）
client.channels.File_Channel.backupCheckpointDir =
# 单事务最大处理10000个事件
client.channels.File_Channel.transactionCapacity = 10000
# 30秒保存一次检查点
client.channels.File_Channel.checkpointInterval = 30000
# 单个数据文件最大2GB
client.channels.File_Channel.maxFileSize = 2146435071
# 最小磁盘空间500MB（建议提升至2GB）
client.channels.File_Channel.minimumRequiredSpace = 524288000
# 线程等待时间3秒
client.channels.File_Channel.keep_alive = 3
# 禁用旧版日志重放机制
client.channels.File_Channel.use-log-replay-v1 = false
# 禁用快速重放模式
client.channels.File_Channel.use-fast-replay = false
# 关闭通道时强制保存检查点
client.channels.File_Channel.checkpointOnClose = true

# === Sink: Kafka_Sink ===
# Sink类型：Kafka生产者
client.sinks.Kafka_Sink.type = org.apache.flume.sink.kafka.KafkaSink
# 严重错误：必须替换为实际Kafka主题名
client.sinks.Kafka_Sink.kafka.topic = kafka.topic
# 每批次发送1000条消息
client.sinks.Kafka_Sink.flumeBatchSize = 1000
# Kafka集群地址（安全风险：建议使用域名）
client.sinks.Kafka_Sink.kafka.bootstrap.servers = ip:port
# 安全协议：SASL明文认证
client.sinks.Kafka_Sink.kafka.security.protocol = SASL_PLAINTEXT
# 拒绝超过长度的消息
client.sinks.Kafka_Sink.ignoreLongMessage = false
# 最大消息长度约1MB
client.sinks.Kafka_Sink.messageMaxLength = 1000012
# 未配置的监控参数
client.sinks.Kafka_Sink.montime =
# 绑定输入通道
client.sinks.Kafka_Sink.channel = File_Channel