##############################################
# FLUME AGENT: client
# 架构：Taildir Source -> Kafka Channel -> HDFS Sink
##############################################
client.sources = Taildir_Source
client.channels = Kafka_Channel
client.sinks = HDFS_Sink

##############################################
# TAILDIR SOURCE 配置 - 监控日志文件增量
##############################################
# 使用Taildir Source实时跟踪文件追加内容
client.sources.Taildir_Source.type = TAILDIR

# 1. 位置文件路径（记录读取偏移量）
# 需确保目录存在且Agent有写权限
client.sources.Taildir_Source.positionFile = /opt/mrs/flume/FlumeClients/testApp1/flume_taildir/taildir_position.json

# 2. 监控行为优化
client.sources.Taildir_Source.skipToEnd = false  # 启动时从上次位置继续读取
client.sources.Taildir_Source.idleTimeout = 60000  # 文件60秒无更新视为空闲（毫秒）
client.sources.Taildir_Source.writePosInterval = 10000  # 10秒写入一次位置信息（毫秒）
client.sources.Taildir_Source.batchSize = 1000  # 每批次最大事件数
client.sources.Taildir_Source.fileHeader = true  # 在Event Header中添加文件路径信息
# 正则拦截器提取文件名（用于HDFS路径或日志分类）
client.sources.Taildir_Source.interceptors = i1
client.sources.Taildir_Source.interceptors.i1.type = regex_extractor
client.sources.Taildir_Source.interceptors.i1.regex = .*/([^/]+\.log)$  # 捕获文件名
client.sources.Taildir_Source.interceptors.i1.serializers = s1
client.sources.Taildir_Source.interceptors.i1.serializers.s1.name = logfile

# 3. 监控文件组定义
# 使用正则匹配.log结尾文件（多目录可用逗号分隔）
client.sources.Taildir_Source.filegroups.f1 = /data/logs/app_logs_flume/ccdlk/pro/testApp1/.*\.log

# 4. 连接通道
client.sources.Taildir_Source.channels = Kafka_Channel


##############################################
# KAFKA CHANNEL 配置 - 高可靠数据缓冲
##############################################
# Kafka Channel替代传统Channel+Sink组合，减少组件依赖
client.channels.Kafka_Channel.type = org.apache.flume.channel.kafka.KafkaChannel

# 1. Kafka集群地址（生产环境建议3节点以上）
client.channels.Kafka_Channel.kafka.bootstrap.servers = ip:port

# 2. Topic与消费者组
client.channels.Kafka_Channel.kafka.topic = ods_log_ccdlk_testApp1_pre  # 指定写入Topic
client.channels.Kafka_Channel.kafka.consumer.group.id = flume_kafka_channel  # 消费组ID（需唯一）

# 3. 数据可靠性
client.channels.Kafka_Channel.kafka.producer.acks = all  # 需所有ISR副本确认
client.channels.Kafka_Channel.kafka.producer.retries = 10  # 生产者重试次数
client.channels.Kafka_Channel.kafka.consumer.auto.offset.reset = earliest  # 无偏移量时从头消费

# 4. 安全认证（SASL_PLAINTEXT适用于Kerberos环境）
client.channels.Kafka_Channel.kafka.producer.security.protocol = SASL_PLAINTEXT
client.channels.Kafka_Channel.kafka.consumer.security.protocol = SASL_PLAINTEXT

# 5. 容量控制（防OOM）
client.channels.Kafka_Channel.messageMaxLength = 2097152  # 单消息最大2MB


##############################################
# HDFS SINK 配置 - 数据持久化存储
##############################################
# HDFS Sink从Kafka消费数据并写入HDFS
client.sinks.HDFS_Sink.type = hdfs

# 1. HDFS存储路径（按日期分区）
# %Y-%m-%d替换原%!Y(MISSING)确保兼容Hive分区
client.sinks.HDFS_Sink.hdfs.path = hdfs://hacluster/dlkdata/dlk_flume_pro/testApp1/load_date=%Y-%m-%d/

# 2. 文件滚动策略（防小文件）
client.sinks.HDFS_Sink.hdfs.rollSize = 134217728  # 128MB滚动（单位：字节）
client.sinks.HDFS_Sink.hdfs.rollInterval = 1800  # 30分钟滚动（单位：秒）
client.sinks.HDFS_Sink.hdfs.rollCount = 0  # 禁用事件数滚动

# 3. 文件命名规则
client.sinks.HDFS_Sink.hdfs.filePrefix = log_  # 文件名前缀
client.sinks.HDFS_Sink.hdfs.fileSuffix = .gz  # 文件压缩后缀
client.sinks.HDFS_Sink.hdfs.inUseSuffix = .tmp  # 写入中临时后缀

# 4. 压缩配置（平衡CPU与存储）
client.sinks.HDFS_Sink.hdfs.fileType = CompressedStream
client.sinks.HDFS_Sink.hdfs.codeC = gzip  # GZIP压缩率高于Snappy

# 5. Kerberos认证（安全集群必配）
client.sinks.HDFS_Sink.hdfs.kerberosPrincipal = dlk_flume_pro  # 认证主体
client.sinks.HDFS_Sink.hdfs.kerberosKeytab = /data/keytabs/dlk_flume_pro.keytab  # Keytab路径（需400权限）

# 6. 性能与容错
client.sinks.HDFS_Sink.hdfs.callTimeout = 30000  # HDFS操作超时（毫秒）
client.sinks.HDFS_Sink.hdfs.threadsPoolSize = 15  # 并发写入线程数
client.sinks.HDFS_Sink.hdfs.useLocalTimeStamp = true  # 使用本地时区非UTC[5](@ref)
client.sinks.HDFS_Sink.hdfs.batchCallTimeout = 60000  # 批量操作超时

# 7. 数据格式化
client.sinks.HDFS_Sink.serializer.appendNewline = true  # 每事件追加换行符

# 8. 连接通道
client.sinks.HDFS_Sink.channel = Kafka_Channel
